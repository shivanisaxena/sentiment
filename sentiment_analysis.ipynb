{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivanisaxena/sentiment/blob/master/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaDn6JY3pP5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.callbacks\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Input, Dropout, MaxPooling1D, Conv1D, GlobalMaxPool1D\n",
        "from keras.layers import LSTM, Lambda, concatenate, TimeDistributed, Bidirectional"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "But1C6dppThw",
        "colab_type": "code",
        "outputId": "7ccf7abe-0bd3-4389-eee5-28e310df7bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW1SRaKOpgKA",
        "colab_type": "code",
        "outputId": "fae24860-a8bf-437e-8052-8c07586c9d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model\n",
        "from keras.layers import LeakyReLU"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5eGTn39pheE",
        "colab_type": "code",
        "outputId": "20cc69fa-97f7-4f2e-ebe2-23a2abcb5d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import re\n",
        "with open('/content/drive/My Drive/sentiment_analysis/train.txt') as fp:\n",
        "\tlineList = fp.readlines()\n",
        "\n",
        "tempTrainData = []\n",
        "tempList = []\n",
        "for item in lineList:\n",
        "\titem = re.sub(r'\\t',' ',item)\n",
        "\titem = re.sub(r'\\n','',item)\n",
        "\ttemp = item.split(' ')\n",
        "\tif len(temp)==3:\n",
        "\t\ttempTrainData.append(tempList)\n",
        "\t\ttempTrainData.append([temp[2]])\n",
        "\t\ttempList = [' ']\n",
        "\telse:\n",
        "\t\ttempList.append(temp[0])\n",
        "\n",
        "trainData = np.array(tempTrainData)\n",
        "\n",
        "dataList = []\n",
        "sentiList = []\n",
        "\n",
        "for i in range(len(trainData)):\n",
        "\ts = ''\n",
        "\tfor item in trainData[i]:\n",
        "\t\ts = s+' '+item\n",
        "\t\n",
        "\tif s == '' or s == ' ':\n",
        "\t\tpass\n",
        "\telif s == ' positive' or s== ' negative' or s==' neutral':\n",
        "\t\ts = re.sub(r'','',s)\n",
        "\t\tsentiList.append(s)\n",
        "\telse:\n",
        "\t\tdataList.append(s)\n",
        "\n",
        "del sentiList[len(sentiList)-1]\n",
        "\n",
        "print(len(sentiList),sentiList[0])\n",
        "print(len(dataList),dataList[0])"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15130  negative\n",
            "15130    @ AdilNisarButt pakistan ka ghra tauq he Pakistan Israel ko tasleem nahein kerta Isko Palestine kehta he - OCCUPIED PALESTINE \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPPPDXvupqnd",
        "colab_type": "code",
        "outputId": "da21b40d-207b-4076-e9af-369c024c08d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "with open('/content/drive/My Drive/sentiment_analysis/test.txt') as fp:\n",
        "\tlineList = fp.readlines()\n",
        "\n",
        "tempTestData = []\n",
        "tempTestList = []\n",
        "for item in lineList:\n",
        "\titem = re.sub(r'\\t',' ',item)\n",
        "\titem = re.sub(r'\\n','',item)\n",
        "\ttemp = item.split(' ')\n",
        "\tif len(temp)==3:\n",
        "\t\ttempTestData.append(tempTestList)\n",
        "\t\ttempTestData.append([temp[2]])\n",
        "\t\ttempTestList = [' ']\n",
        "\telse:\n",
        "\t\ttempTestList.append(temp[0])\n",
        "\n",
        "testData = np.array(tempTestData)\n",
        "\n",
        "dataTestList = []\n",
        "sentiTestList = []\n",
        "\n",
        "for i in range(len(testData)):\n",
        "\ts = ''\n",
        "\tfor item in testData[i]:\n",
        "\t\ts = s+' '+item\n",
        "\t\n",
        "\tif s == '' or s == ' ':\n",
        "\t\tpass\n",
        "\telif s == ' positive' or s== ' negative' or s==' neutral':\n",
        "\t\ts = re.sub(r'','',s)\n",
        "\t\tsentiTestList.append(s)\n",
        "\telse:\n",
        "\t\tdataTestList.append(s)\n",
        "\n",
        "del sentiTestList[len(sentiTestList)-1]\n",
        "\n",
        "train_arr=dataList\n",
        "test_arr=dataTestList\n",
        "train_arr_senti=sentiList\n",
        "test_arr_senti=sentiTestList\n",
        "\n",
        "print(len(test_arr),test_arr[0])\n",
        "print(len(test_arr_senti),test_arr_senti[0])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1868    RT @ UAAPconfessions Love looks good on Maddie !!! Ako lang ba yung sobrang masaya kasi may zolo sya ? Before with the past Z medyo lowkey s … \n",
            "1868  neutral\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvAhp8rbp2aJ",
        "colab_type": "code",
        "outputId": "3f1e2baa-4fae-4f95-c61a-7299874b9630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "\n",
        "train_texts = train_arr[:]\n",
        "train_texts = [s.lower() for s in train_texts]\n",
        "\n",
        "test_texts = test_arr[:]\n",
        "test_texts = [s.lower() for s in test_texts]\n",
        "\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(train_texts)\n",
        "\n",
        "train_sequences = tk.texts_to_sequences(train_texts)\n",
        "test_texts = tk.texts_to_sequences(test_texts)\n",
        "train_data = pad_sequences(train_sequences, maxlen=170, padding='post')\n",
        "test_data = pad_sequences(test_texts, maxlen=170, padding='post')\n",
        "train_data = np.array(train_data)\n",
        "test_data = np.array(test_data)\n",
        "train_classes_1 = train_arr_senti[:]\n",
        "train_class_list = []\n",
        "\n",
        "for i in train_classes_1:\n",
        "  if(i == ' positive'):\n",
        "    train_class_list.append(0)\n",
        "  elif(i == ' negative'):\n",
        "    train_class_list.append(2)\n",
        "  else:\n",
        "    train_class_list.append(1)\n",
        "\n",
        "train_class_list = np.array(train_class_list)\n",
        "\n",
        "test_classes_1 = test_arr_senti[:]\n",
        "test_class_list = []\n",
        "\n",
        "for i in test_classes_1:\n",
        "  if(i == ' positive'):\n",
        "    test_class_list.append(0)\n",
        "  elif(i == ' negative'):\n",
        "    test_class_list.append(2)\n",
        "  else:\n",
        "    test_class_list.append(1)\n",
        "\n",
        "test_class_list = np.array(test_class_list)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_classes = to_categorical(train_class_list)\n",
        "test_classes = to_categorical(test_class_list)\n",
        "print(train_classes)\n",
        "print(test_classes)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " ...\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7XEcE4_p-JV",
        "colab_type": "code",
        "outputId": "bfece62e-bfda-44c4-b8bf-86c6b6a590d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# char CNN\n",
        "input_size = 170\n",
        "vocab_size = len(tk.word_index)\n",
        "print(tk.word_index)\n",
        "embedding_size = 899\n",
        "conv_layers = [[256, 3, 3],\n",
        "               [256, 3, 3],\n",
        "               [256, 3, -1],\n",
        "               [256, 3, -1],\n",
        "               [256, 3, -1],\n",
        "               [256, 3, 3]]\n",
        "\n",
        "fully_connected_layers = [1024, 1024]\n",
        "num_of_classes = 3\n",
        "dropout_p = 0.5\n",
        "optimizer = 'Adamax'\n",
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "# Embedding weights\n",
        "embedding_weights = [] \n",
        "embedding_weights.append(np.zeros(vocab_size))  \n",
        "\n",
        "for char, i in tk.word_index.items():\n",
        "    onehot = np.zeros(vocab_size)\n",
        "    onehot[i - 1] = 1\n",
        "    embedding_weights.append(onehot)\n",
        "\n",
        "embedding_weights = np.array(embedding_weights)\n",
        "embedding_layer = Embedding(vocab_size+1,embedding_size, weights=[embedding_weights])\n",
        "inputs = Input(shape=(input_size,), name='input', dtype='int64') \n",
        "x = embedding_layer(inputs)\n",
        "for filter_num, filter_size, pooling_size in conv_layers:\n",
        "    x = Conv1D(filter_num, filter_size)(x)\n",
        "    x = LeakyReLU(alpha=0.05)(x)\n",
        "    if pooling_size != -1:\n",
        "        x = MaxPooling1D(pool_size=pooling_size)(x)  \n",
        "\n",
        "x = Flatten()(x)\n",
        "\n",
        "for dense_size in fully_connected_layers:\n",
        "    x = Dense(dense_size, activation='linear')(x)\n",
        "    x = LeakyReLU(alpha=0.05)(x)\n",
        "    x = Dropout(dropout_p)(x)\n",
        "\n",
        "predictions = Dense(num_of_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy']) \n",
        "model.summary()\n",
        "\n",
        "x_test = test_data\n",
        "y_test = test_classes\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'UNK': 1, ' ': 2, 'a': 3, 'i': 4, 'e': 5, 'h': 6, 't': 7, 'o': 8, 'r': 9, 's': 10, 'n': 11, 'k': 12, 'm': 13, 'l': 14, 'd': 15, 'u': 16, 'p': 17, 'b': 18, 'y': 19, 'c': 20, 'g': 21, '/': 22, '@': 23, '.': 24, 'j': 25, 'w': 26, 'f': 27, 'v': 28, '…': 29, 'z': 30, '_': 31, '1': 32, '0': 33, 'q': 34, '2': 35, 'x': 36, '3': 37, '9': 38, '5': 39, '4': 40, '7': 41, '8': 42, '6': 43, '#': 44, '!': 45, '😂': 46, '?': 47, \"'\": 48, '-': 49, '’': 50, '️': 51, '🙏': 52, '❤': 53, '🤣': 54, '😍': 55, '&': 56, ')': 57, '(': 58, '😭': 59, '*': 60, '😘': 61, '🇮': 62, '🇳': 63, '😊': 64, '🌹': 65, '💜': 66, '🎂': 67, '“': 68, '💕': 69, 'ा': 70, '😁': 71, '👏': 72, '🎉': 73, '💖': 74, '👍': 75, '|': 76, '👌': 77, '”': 78, '✌': 79, '😜': 80, '%': 81, 'र': 82, '😆': 83, '💐': 84, '🏻': 85, '+': 86, '♥': 87, '🤗': 88, '्': 89, '😎': 90, '😡': 91, '🙄': 92, '🔥': 93, '~': 94, '🤔': 95, '\\u200d': 96, '😉': 97, '💙': 98, '😠': 99, '😅': 100, '🙌': 101, '\\U0001f92a': 102, '😀': 103, 'न': 104, 'क': 105, 'म': 106, ';': 107, '🌸': 108, '💓': 109, 'ी': 110, '😋': 111, 'स': 112, '💔': 113, '☺': 114, '‘': 115, '😢': 116, '😌': 117, '—': 118, '\\U0001f970': 119, 'े': 120, '💞': 121, '💪': 122, '😝': 123, 'ह': 124, '😔': 125, 'त': 126, '💯': 127, '💗': 128, '😒': 129, '😑': 130, '😄': 131, '\\xa0': 132, '🙈': 133, '[': 134, '🚩': 135, '🌷': 136, '🎊': 137, 'ं': 138, '😇': 139, '=': 140, '😪': 141, '👇': 142, '🌺': 143, '😹': 144, 'é': 145, ']': 146, 'य': 147, '😛': 148, 'ि': 149, '😐': 150, '🏼': 151, '🙂': 152, '>': 153, '😬': 154, '🎁': 155, '😃': 156, 'ग': 157, 'प': 158, 'ब': 159, '\\U0001f929': 160, '💚': 161, '😤': 162, '😏': 163, 'á': 164, '🎈': 165, '🏽': 166, '😕': 167, '$': 168, '💏': 169, 'ا': 170, '🏆': 171, '😩': 172, '🤦': 173, '<': 174, 'द': 175, '🇵': 176, '🇰': 177, '\"': 178, '♂': 179, 'ā': 180, 'व': 181, '\\U0001f928': 182, '♀': 183, '🙊': 184, 'ज': 185, '☹': 186, '💃': 187, 'ु': 188, 'ल': 189, '\\U0001f97a': 190, '🏾': 191, '🖕': 192, '🤧': 193, 'ो': 194, 'ू': 195, '💝': 196, '✨': 197, 'ै': 198, '\\U0001f92c': 199, '✅': 200, 'ل': 201, '💋': 202, '❣': 203, 'श': 204, '🌈': 205, '🌚': 206, 'م': 207, 'í': 208, '😶': 209, '😈': 210, 'ر': 211, '😷': 212, '👉': 213, '💛': 214, '💘': 215, '–': 216, '😣': 217, 'भ': 218, '😳': 219, 'ع': 220, '💮': 221, '🔯': 222, '👋': 223, '😨': 224, '😞': 225, '\\U0001f973': 226, 'ı': 227, 'ی': 228, '🤓': 229, '🏵': 230, '👑': 231, '😥': 232, 'ş': 233, '😚': 234, 'ᴇ': 235, 'و': 236, 'अ': 237, '🌼': 238, '😴': 239, '`': 240, '🤘': 241, '👀': 242, '\\U000e0067': 243, '🖤': 244, '⚫': 245, '😓': 246, '🙉': 247, '🎶': 248, '💥': 249, '👫': 250, 'ç': 251, '🏏': 252, 'د': 253, 'फ': 254, '🌲': 255, '్': 256, '‼': 257, '😻': 258, '🌱': 259, 'å': 260, 'ü': 261, '🐍': 262, '🤷': 263, '💫': 264, 'ध': 265, '🦋': 266, 'ट': 267, '\\U0001f932': 268, '👻': 269, '\\U0001f9e1': 270, 'ñ': 271, 'ê': 272, '🍫': 273, '🤞': 274, '😫': 275, '🙇': 276, 'ᴀ': 277, '🎼': 278, '🙃': 279, 'च': 280, '✊': 281, '💑': 282, 'ख': 283, '\\U0001f92d': 284, '👦': 285, '👈': 286, '🎵': 287, '👩': 288, '➡': 289, '✖': 290, '✔': 291, '😱': 292, 'थ': 293, '🗿': 294, '🤙': 295, 'ة': 296, 'ق': 297, 'ᴛ': 298, '🌿': 299, '🙆': 300, 'ए': 301, '•': 302, '🌵': 303, 'ర': 304, '´': 305, '⛳': 306, 'ड': 307, 'ج': 308, '☝': 309, '🇦': 310, '🏴': 311, '\\U000e0062': 312, '\\U000e007f': 313, 'ã': 314, '😯': 315, '❓': 316, '❗': 317, '🍧': 318, '₹': 319, '📄': 320, '🎓': 321, 'ɪ': 322, 'ɴ': 323, 'ᴍ': 324, '✋': 325, 'ت': 326, 'ک': 327, '🤝': 328, '👭': 329, '😟': 330, '🍰': 331, '😮': 332, 'ి': 333, 'ా': 334, '🔫': 335, '👎': 336, '🐱': 337, '🏳': 338, '😰': 339, '\\U000e0065': 340, '\\U000e006e': 341, '🖐': 342, '🌙': 343, '{': 344, '}': 345, '💩': 346, '่': 347, 'ई': 348, '^': 349, 'س': 350, '⬇': 351, '\\U0001f92f': 352, '¡': 353, '╭': 354, '╮': 355, '👨': 356, 'ɩ': 357, 'ᵒ': 358, 'ᵉ': 359, 'の': 360, '✍': 361, '𝗲': 362, '😗': 363, '🎇': 364, 'ä': 365, 'ष': 366, '👰': 367, '👸': 368, 'ʀ': 369, 'ô': 370, 'ł': 371, '📃': 372, '\\U0001f9da': 373, '👿': 374, 'इ': 375, '🇺': 376, '🇸': 377, '🍾': 378, 'ौ': 379, 'క': 380, 'ు': 381, 'స': 382, 'ం': 383, 'డ': 384, 'ప': 385, '🐷': 386, '😙': 387, '🇫': 388, '🇩': 389, '🇿': 390, '👊': 391, '🇬': 392, '💸': 393, 'आ': 394, 'ค': 395, '़': 396, '🚨': 397, '🏿': 398, '\\U0001f91f': 399, 'ے': 400, 'č': 401, 'ý': 402, '\\U0001f9d0': 403, '🔴': 404, '।': 405, '🙁': 406, '❌': 407, '👠': 408, '🎤': 409, '𝄞': 410, 'ਂ': 411, 'ن': 412, '🌟': 413, '⬆': 414, '🌻': 415, '🎀': 416, '🔝': 417, 'ˢ': 418, '🗣': 419, '\\u2066': 420, '💎': 421, 'ン': 422, 'ー': 423, '\\u200b': 424, '𝘀': 425, '🐖': 426, '😖': 427, 'ạ': 428, 'ẽ': 429, 'ó': 430, '🍨': 431, '🍬': 432, '🐯': 433, '🌝': 434, '👾': 435, 'ㅠ': 436, 'ي': 437, '►': 438, 'ण': 439, 'ᴋ': 440, 'ғ': 441, 'ᴅ': 442, '💬': 443, 'ᴏ': 444, 'ᴘ': 445, 'đ': 446, '💡': 447, '💁': 448, '🤤': 449, 'ب': 450, 'ż': 451, '🙋': 452, 'छ': 453, 'ﷺ': 454, '📷': 455, 'ँ': 456, '⃣': 457, '스': 458, 'ｓ': 459, 'మ': 460, 'న': 461, 'ద': 462, 'త': 463, 'ో': 464, 'య': 465, 'ల': 466, 'ఉ': 467, 'ష': 468, 'ृ': 469, 'ů': 470, '🇧': 471, '🇱': 472, '🌴': 473, '✈': 474, '💌': 475, '🎋': 476, 'ۃ': 477, '💀': 478, 'ꕊ': 479, '𝐡': 480, '𝐞': 481, 'ผ': 482, 'น': 483, 'ป': 484, 'ี': 485, 'อ': 486, 'บ': 487, 'ุ': 488, 'ท': 489, 'ก': 490, '\\U0001f975': 491, '🚬': 492, '🦉': 493, 'ش': 494, '👧': 495, '🌍': 496, 'ः': 497, '💅': 498, '🤒': 499, '｡': 500, '‿': 501, '◉': 502, '🍦': 503, 'ö': 504, '×': 505, '🚴': 506, '🚶': 507, '🎧': 508, '🐼': 509, '🐂': 510, '🌶': 511, 'ř': 512, '🍍': 513, '🌎': 514, '·': 515, '¯': 516, 'っ': 517, '˘': 518, '̩': 519, '🍎': 520, '🍏': 521, '♡': 522, '⃑': 523, 'լ': 524, 'ɣ': 525, 'є': 526, 'ɲ': 527, 'ਹ': 528, 'ੁ': 529, 'ਤ': 530, 'ੀ': 531, 'ਰ': 532, '🦍': 533, 'ú': 534, '🌏': 535, '®': 536, '🎬': 537, '👺': 538, '기': 539, '다': 540, '⠀': 541, '🇭': 542, '〣': 543, 'º': 544, 'ᵐ': 545, 'ʸ': 546, 'ᵍ': 547, 'ᵘ': 548, 'ᵃ': 549, 'ᶦ': 550, '\\u2069': 551, 'ダ': 552, 'ヒ': 553, 'ョ': 554, '日': 555, '限': 556, '定': 557, 'で': 558, 'す': 559, '𝗶': 560, '𝗰': 561, '🍃': 562, 'ī': 563, 'ē': 564, '🌘': 565, '⤴': 566, 'а': 567, 'î': 568, '✴': 569, '❔': 570, '⁉': 571, '➕': 572, '¿': 573, 'ğ': 574, '🤠': 575, '☪': 576, '🏖': 577, '👼': 578, 'ʟ': 579, '👥': 580, 'ɢ': 581, '💾': 582, 'ᴠ': 583, 'ᴄ': 584, 'ʏ': 585, 'ᴜ': 586, 'ʜ': 587, 'ế': 588, 'ồ': 589, 'ủ': 590, 'ề': 591, 'ố': 592, '🍂': 593, 'ئ': 594, 'ć': 595, 'ę': 596, 'ś': 597, '↑': 598, '🔷': 599, '🔶': 600, '🐸': 601, '👆': 602, '✉': 603, '🇲': 604, '🛐': 605, 'ﷻ': 606, '\\U0001f9c1': 607, '🥂': 608, '𝐹': 609, '𝒾': 610, '𝑔': 611, '𝒽': 612, '𝓉': 613, '𝒻': 614, '𝑜': 615, '𝓇': 616, '𝓊': 617, '𝓈': 618, '📝': 619, '猫': 620, 'ね': 621, 'こ': 622, 'ネ': 623, 'コ': 624, '🔞': 625, '💣': 626, 'ऑ': 627, '한': 628, '국': 629, '댄': 630, '러': 631, '시': 632, '팀': 633, '약': 634, '자': 635, '제': 636, '일': 637, '은': 638, '행': 639, 'ｕ': 640, 'ｎ': 641, 'ｅ': 642, 'ｔ': 643, '🐈': 644, '🤚': 645, 'ధ': 646, 'బ': 647, 'ే': 648, 'ఇ': 649, 'అ': 650, 'ూ': 651, 'వ': 652, 'చ': 653, '🍇': 654, 'ě': 655, 'š': 656, 'ठ': 657, '🏁': 658, '🥃': 659, '🎩': 660, '🦁': 661, '🕋': 662, '🐗': 663, 'ه': 664, '🕺': 665, '🕯': 666, '🙀': 667, 'ॉ': 668, '😲': 669, '🔚': 670, '🔨': 671, '🐒': 672, '𝐂': 673, '𝐚': 674, '𝐩': 675, '𝐭': 676, '𝐫': 677, '𝟔': 678, '𝟏': 679, '𝟐': 680, '𝐉': 681, '𝐮': 682, '𝐧': 683, '𝐖': 684, '𝐢': 685, '𝐬': 686, '︎': 687, 'า': 688, 'ไ': 689, 'แ': 690, 'ล': 691, '้': 692, 'ว': 693, 'ร': 694, 'ึ': 695, 'ง': 696, 'ข': 697, 'ณ': 698, 'ย': 699, 'ู': 700, 'ั': 701, '🤑': 702, 'ں': 703, 'ہ': 704, '★': 705, '⚔': 706, '🇷': 707, '☀': 708, '\\U0001f9f8': 709, 'ň': 710, '👐': 711, '🍀': 712, '🙅': 713, '⚪': 714, '\\U000e0073': 715, '\\U000e0063': 716, '\\U000e0074': 717, '🛍': 718, 'ò': 719, '£': 720, '€': 721, '⭐': 722, 'à': 723, '🦄': 724, '🍱': 725, '⚽': 726, '👞': 727, '\\U0001f97e': 728, '👟': 729, '😼': 730, '🌊': 731, '℅': 732, '🐥': 733, '🐿': 734, '🔈': 735, '🔪': 736, '⌚': 737, '▂': 738, '🍟': 739, '🏋': 740, '🐶': 741, '¥': 742, 'ਣ': 743, 'ਸ': 744, 'ਨ': 745, 'ਕ': 746, 'ੋ': 747, 'ਗ': 748, 'ੇ': 749, 'ਾ': 750, 'ਫ': 751, 'ਿ': 752, '―': 753, 'ì': 754, '🇨': 755, '🌳': 756, 'غ': 757, 'ط': 758, 'خ': 759, 'ز': 760, '🐮': 761, '🔁': 762, '\\U0001f9b3': 763, '📻': 764, '🌄': 765, '☔': 766, 'μ': 767, '🐓': 768, '❇': 769, '🇪': 770, 'ঈ': 771, 'দ': 772, 'ম': 773, 'ো': 774, 'ব': 775, 'া': 776, 'র': 777, 'ক': 778, '°': 779, '이': 780, '나': 781, '경': 782, '🐕': 783, '👅': 784, 'گ': 785, '🔔': 786, '🔐': 787, '🤡': 788, '🐐': 789, '\\U0001f96d': 790, '👹': 791, '📿': 792, '멀': 793, '티': 794, '뷰': 795, '와': 796, '함': 797, '께': 798, '즐': 799, '는': 800, '머': 801, '터': 802, '실': 803, '황': 804, '생': 805, '중': 806, '계': 807, '만': 808, '립': 809, '니': 810, '😵': 811, '🎟': 812, '🕌': 813, '💟': 814, 'औ': 815, '🏡': 816, '⛲': 817, 'δ': 818, 'ʰ': 819, 'ᵈ': 820, 'ʳ': 821, 'ᶜ': 822, 'ᵗ': 823, 'ᵛ': 824, 'ᵏ': 825, '😿': 826, '🤕': 827, '🎥': 828, '🤐': 829, '🔟': 830, '🎆': 831, '🎗': 832, 'は': 833, '🦅': 834, '誕': 835, '生': 836, '当': 837, '、': 838, 'イ': 839, 'ベ': 840, 'ト': 841, 'ス': 842, 'テ': 843, 'ジ': 844, 'が': 845, 'つ': 846, 'い': 847, 'に': 848, '開': 849, '放': 850, 'べ': 851, 'て': 852, 'パ': 853, 'ズ': 854, 'ル': 855, 'を': 856, 'ク': 857, 'リ': 858, 'ア': 859, 'る': 860, 'と': 861, 'ム': 862, 'ビ': 863, 'ま': 864, 'ỉ': 865, 'ư': 866, 'ờ': 867, '⇒': 868, '⛽': 869, '👶': 870, '𝗣': 871, '𝘂': 872, '𝗯': 873, '𝗹': 874, '𝗿': 875, '𝘃': 876, '𝗺': 877, '𝗮': 878, '𝗴': 879, '😦': 880, '™': 881, '📈': 882, '⚕': 883, '📣': 884, 'м': 885, 'о': 886, 'д': 887, 'к': 888, 'р': 889, 'т': 890, '🇾': 891, 'ص': 892, '🌌': 893, '👒': 894, '🥇': 895, '⚘': 896, '💍': 897, '\\U0001f974': 898, '\\U0001f92e': 899}\n",
            "Model: \"model_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 170)               0         \n",
            "_________________________________________________________________\n",
            "embedding_26 (Embedding)     (None, 170, 899)          809100    \n",
            "_________________________________________________________________\n",
            "conv1d_145 (Conv1D)          (None, 168, 256)          690688    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_187 (LeakyReLU)  (None, 168, 256)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_74 (MaxPooling (None, 56, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_146 (Conv1D)          (None, 54, 256)           196864    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_188 (LeakyReLU)  (None, 54, 256)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_75 (MaxPooling (None, 18, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_147 (Conv1D)          (None, 16, 256)           196864    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_189 (LeakyReLU)  (None, 16, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_148 (Conv1D)          (None, 14, 256)           196864    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_190 (LeakyReLU)  (None, 14, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_149 (Conv1D)          (None, 12, 256)           196864    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_191 (LeakyReLU)  (None, 12, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_150 (Conv1D)          (None, 10, 256)           196864    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_192 (LeakyReLU)  (None, 10, 256)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_76 (MaxPooling (None, 3, 256)            0         \n",
            "_________________________________________________________________\n",
            "flatten_24 (Flatten)         (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 1024)              787456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_193 (LeakyReLU)  (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_194 (LeakyReLU)  (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 3)                 3075      \n",
            "=================================================================\n",
            "Total params: 4,324,239\n",
            "Trainable params: 4,324,239\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fwj-t4_qGcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.callbacks import Callback\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "class Metrics(Callback):\n",
        "  def __init__(self, logs={}):\n",
        "    self.val_f1s = []\n",
        "    self.val_recalls = []\n",
        "    self.val_precisions = []\n",
        " \n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    val_predict = (np.asarray(self.model.predict(x_test))).round()\n",
        "    val_targ = y_test\n",
        "    _val_f1 = f1_score(val_targ, val_predict,average=None)\n",
        "    _val_recall = recall_score(val_targ, val_predict,average=None)\n",
        "    _val_precision = precision_score(val_targ, val_predict,average=None)\n",
        "    self.val_f1s.append(_val_f1)\n",
        "    self.val_recalls.append(_val_recall)\n",
        "    self.val_precisions.append(_val_precision)\n",
        "    print(\" f1_score_Class1: %f \\t f1_score_Class2: %f \\t f1_score_Class3: %f \\n precision_Class1: %f \\t precision_Class2: %f \\t precision_Class3: %f \\n recall_Class1 %f \\t recall_Class2 %f \\t recall_Class3 %f\" %(_val_f1[0], _val_f1[1], _val_f1[2], _val_precision[0], _val_precision[1], _val_precision[2], _val_recall[0], _val_recall[1], _val_recall[2]))\n",
        "    return\n",
        " \n",
        "metrics = Metrics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RWsR1-JqMp1",
        "colab_type": "code",
        "outputId": "b6f5471e-366f-449a-9a29-c8dd26fba636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "model.fit(x_train, y_train,validation_data=(x_test, y_test),batch_size=64,epochs=8,verbose=1,callbacks=[metrics])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15130 samples, validate on 1868 samples\n",
            "Epoch 1/8\n",
            "15130/15130 [==============================] - 325s 21ms/step - loss: 1.0560 - acc: 0.4341 - val_loss: 1.0184 - val_acc: 0.4732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " f1_score_Class1: 0.445328 \t f1_score_Class2: 0.000000 \t f1_score_Class3: 0.176183 \n",
            " precision_Class1: 0.528302 \t precision_Class2: 0.000000 \t precision_Class3: 0.666667 \n",
            " recall_Class1 0.384880 \t recall_Class2 0.000000 \t recall_Class3 0.101504\n",
            "Epoch 2/8\n",
            "15130/15130 [==============================] - 316s 21ms/step - loss: 0.9454 - acc: 0.5234 - val_loss: 1.0014 - val_acc: 0.4946\n",
            " f1_score_Class1: 0.404281 \t f1_score_Class2: 0.000000 \t f1_score_Class3: 0.519126 \n",
            " precision_Class1: 0.656371 \t precision_Class2: 0.000000 \t precision_Class3: 0.503534 \n",
            " recall_Class1 0.292096 \t recall_Class2 0.000000 \t recall_Class3 0.535714\n",
            "Epoch 3/8\n",
            "15130/15130 [==============================] - 319s 21ms/step - loss: 0.8902 - acc: 0.5614 - val_loss: 0.9748 - val_acc: 0.5241\n",
            " f1_score_Class1: 0.526515 \t f1_score_Class2: 0.015707 \t f1_score_Class3: 0.298817 \n",
            " precision_Class1: 0.586498 \t precision_Class2: 0.600000 \t precision_Class3: 0.701389 \n",
            " recall_Class1 0.477663 \t recall_Class2 0.007958 \t recall_Class3 0.189850\n",
            "Epoch 4/8\n",
            "15130/15130 [==============================] - 322s 21ms/step - loss: 0.8450 - acc: 0.5970 - val_loss: 1.0010 - val_acc: 0.5364\n",
            " f1_score_Class1: 0.566164 \t f1_score_Class2: 0.375538 \t f1_score_Class3: 0.502165 \n",
            " precision_Class1: 0.552288 \t precision_Class2: 0.535627 \t precision_Class3: 0.591837 \n",
            " recall_Class1 0.580756 \t recall_Class2 0.289125 \t recall_Class3 0.436090\n",
            "Epoch 5/8\n",
            "15130/15130 [==============================] - 322s 21ms/step - loss: 0.7977 - acc: 0.6288 - val_loss: 0.9986 - val_acc: 0.5294\n",
            " f1_score_Class1: 0.567944 \t f1_score_Class2: 0.238994 \t f1_score_Class3: 0.575729 \n",
            " precision_Class1: 0.575972 \t precision_Class2: 0.570000 \t precision_Class3: 0.576271 \n",
            " recall_Class1 0.560137 \t recall_Class2 0.151194 \t recall_Class3 0.575188\n",
            "Epoch 6/8\n",
            "15130/15130 [==============================] - 318s 21ms/step - loss: 0.7462 - acc: 0.6600 - val_loss: 1.0110 - val_acc: 0.5203\n",
            " f1_score_Class1: 0.558952 \t f1_score_Class2: 0.528624 \t f1_score_Class3: 0.301775 \n",
            " precision_Class1: 0.568384 \t precision_Class2: 0.488739 \t precision_Class3: 0.708333 \n",
            " recall_Class1 0.549828 \t recall_Class2 0.575597 \t recall_Class3 0.191729\n",
            "Epoch 7/8\n",
            "15130/15130 [==============================] - 323s 21ms/step - loss: 0.6798 - acc: 0.6958 - val_loss: 1.1105 - val_acc: 0.5252\n",
            " f1_score_Class1: 0.481707 \t f1_score_Class2: 0.513376 \t f1_score_Class3: 0.538168 \n",
            " precision_Class1: 0.589552 \t precision_Class2: 0.493873 \t precision_Class3: 0.546512 \n",
            " recall_Class1 0.407216 \t recall_Class2 0.534483 \t recall_Class3 0.530075\n",
            "Epoch 8/8\n",
            "15130/15130 [==============================] - 322s 21ms/step - loss: 0.5855 - acc: 0.7516 - val_loss: 1.1401 - val_acc: 0.5252\n",
            " f1_score_Class1: 0.565540 \t f1_score_Class2: 0.437547 \t f1_score_Class3: 0.507553 \n",
            " precision_Class1: 0.543582 \t precision_Class2: 0.509700 \t precision_Class3: 0.546638 \n",
            " recall_Class1 0.589347 \t recall_Class2 0.383289 \t recall_Class3 0.473684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f003781ceb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    }
  ]
}